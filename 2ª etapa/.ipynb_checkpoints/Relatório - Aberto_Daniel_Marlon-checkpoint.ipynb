{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#PageRank com autodecomposições\n",
    "###Grupo: Alberto da Silva, Daniel Santos, Marlon Costa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##O que é PageRank?\n",
    "\n",
    "O PageRank foi desenvolvido por volta de 1998 na Universidade de Stanford por Larry Page (daí o nome “Page” Rank) e Sergey Brin. Foi o primeiro algoritmo utilizado pelo Google para classificar páginas da internet. Tem como objetivo avaliar a relevância de uma determinada página em um contexto de pesquisa. A idéia por trás do PageRank se baseia em uma rede citações podendo ser representada por um grafo, onde cada nó representa uma página Web e a ligação entre as páginas se dá por meio de uma referência que uma página faz a outra. \n",
    "\n",
    "![Grafo de Representação da Rede](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fb/PageRanks-Example.svg/400px-PageRanks-Example.svg.png)\n",
    "\n",
    "\n",
    "Uma característica interessante nesse algoritmo é que o valor de PageRank de um determinado Website não se dá apenas pelo número de citações mas sim pela relevância que eles possuem. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##PageRank em termos de álgebra linear:\n",
    "\n",
    "![Grafo de Páginas](http://pi.math.cornell.edu/~mec/Winter2009/RalucaRemus/Lecture3/Images/graf1.PNG)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![Matriz A](http://pi.math.cornell.edu/~mec/Winter2009/RalucaRemus/Lecture3/Images/matrix.gif)\n",
    "                                    \n",
    "\n",
    "Cada página transfere de forma uniforme sua importância para as páginas às quais ele está vinculado. O nó 1 possui 3 arestas de saída, portanto, ele passará  sua importância para cada um dos outros 3 nós (1/3 para cada). O nó 3 tem apenas uma aresta de saída, assim passa toda a sua importância para o nó 1.\n",
    "\n",
    "O PageRank cria um vetor V de ranks, com um elemento para cada página. Também cria uma matriz de links A, onde relaciona cada página com a citação das demais.\n",
    "\n",
    "Inicialmente atribui valores de PageRank iguais para cada nó e a cada iteração é criado um novo vetor de rankings levando em conta os novos valores de relevância de cada nó.\n",
    "\n",
    "![](http://pi.math.cornell.edu/~mec/Winter2009/RalucaRemus/Lecture3/Images/rezultate_1.gif)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Implementação do PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.18588553],\n",
      "       [0.17111776],\n",
      "       [0.37272084],\n",
      "       [0.27027587]]), array([[0.46917996],\n",
      "       [0.09016757],\n",
      "       [0.27775986],\n",
      "       [0.16289262]]), array([[0.34282525],\n",
      "       [0.17043433],\n",
      "       [0.2779849 ],\n",
      "       [0.20875554]]), array([[0.36250828],\n",
      "       [0.13463382],\n",
      "       [0.29578952],\n",
      "       [0.20706841]]), array([[0.37692517],\n",
      "       [0.14021068],\n",
      "       [0.28543413],\n",
      "       [0.19743005]]), array([[0.36402679],\n",
      "       [0.14429547],\n",
      "       [0.28779278],\n",
      "       [0.20388501]]), array([[0.36877499],\n",
      "       [0.14064093],\n",
      "       [0.28861763],\n",
      "       [0.2019665 ]]), array([[0.36866075],\n",
      "       [0.14198625],\n",
      "       [0.28759441],\n",
      "       [0.20175864]]), array([[0.36770267],\n",
      "       [0.14195388],\n",
      "       [0.28804546],\n",
      "       [0.20229804]]), array([[0.36831531],\n",
      "       [0.14168243],\n",
      "       [0.28798949],\n",
      "       [0.20201283]])]\n"
     ]
    }
   ],
   "source": [
    "def pagerank(M, eps=1.0e-8, d=0.85):\n",
    "    N = M.shape[1]\n",
    "    v = np.random.rand(N, 1)\n",
    "    v = v / np.linalg.norm(v, 1)\n",
    "    last_v = np.ones((N, 1), dtype=np.float32) * 100\n",
    "    M_hat = (d * M) + (((1 - d) / N) * np.ones((N, N), dtype=np.float32))\n",
    "    \n",
    "    while np.linalg.norm(v - last_v, 2) > eps:\n",
    "        last_v = v\n",
    "        v = np.matmul(M_hat, v)\n",
    "        yield  v\n",
    "        \n",
    "\n",
    "\n",
    "M = np.array([[0, 0, 1, 1/2],\n",
    "              [1/3, 0, 0, 0],\n",
    "              [1/3, 1/2, 0, 1/2],\n",
    "              [1/3, 1/2, 0, 0]])\n",
    "\n",
    "v = pagerank(M, 0.001, 0.85)\n",
    "print([i for i in v])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##O Poder dos Autovetores\n",
    "\n",
    "Uma aplicação dos autovetores é a diagonalização de uma matriz. Dados os autovalores e autovetores de uma matriz, você pode criar três matrizes que quando combinadas, correspondem à matriz original. A matriz do meio lida com o escalonamento e é uma matriz diagonal, tendo os autovalores da matriz original como entradas.\n",
    "\n",
    "Isso nos permite algo incrível: exponenciar a matriz original é equivalente a exponenciar *apenas* a matriz do meio do trio de matrizes vindas da diagonalização. E por fim, exponenciar uma matriz diagonal é equivalente à exponenciar *apenas* as entradas da diagonal principal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##SVD\n",
    "\n",
    "Em álgebra linear, a decomposição em valores singulares ou singular value decomposition (SVD) é a fatoração de uma matriz real ou complexa, com diversas aplicações importantes em processamento de sinais e estatística.\n",
    "\n",
    "Formalmente, a decomposição em valores singulares de uma matriz m×n real ou complexa M é uma fatoração ou fatorização na forma:\n",
    "\n",
    "\\begin{equation}{M=U\\Sigma V^{*},}\\end{equation}\n",
    "\n",
    "onde U é uma matriz unitária m×m real ou complexa, Σ é uma matriz retangular diagonal m×n com números reais não-negativos na diagonal, e V* (a conjugada transposta de V) é uma matriz unitária n×n real ou complexa. As entradas diagonais Σi,i de Σ são os chamados valores singulares de M. As m colunas de U e as n colunas de V são os chamados vetores singulares à esquerda e vetores singulares à direita de M, respetivamente.\n",
    "\n",
    "A decomposição em valores singulares e a decomposição em autovalores são intimamente relacionadas. Mais especificamente:\n",
    "\n",
    "\n",
    "- Os vetores singulares à esquerda de M são autovetores de ${MM^{*}.}$\n",
    "- Os vetores singulares à direita de M são autovetores de ${M^{*}M.}$\n",
    "- Os valores singulares não-nulos de M (ao longo da diagonal de Σ) são as raízes quadradas dos autovalores não-nulos de ${M^{*}M}$ ou ${MM^{*}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##Método das potências<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em matemática, o método das potências é um algoritmo para calcular autovalores: dada uma matriz A, o algoritmo irá produzir um número λ (o autovalor) e um vetor v não nulo (o autovetor), tal que Av = λv. O algoritmo também é conhecido como a iteração de Von Mises.\n",
    "\n",
    "O método da potência é um algoritmo muito simples. Ele não computa a decomposição matricial, e portanto pode ser usada quando A é uma grande matriz esparsa. No entanto, ele irá encontrar apenas um autovalor (aquele com o maior módulo) e poderá convergir lentamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.72101012, 0.24033671, 0.54075759, 0.36050506])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"https://en.wikipedia.org/wiki/Power_iteration\"\"\"\n",
    "\n",
    "\n",
    "def power_iteration(A, num_simulations = 100):\n",
    "    # Ideally choose a random vector\n",
    "    # To decrease the chance that our vector\n",
    "    # Is orthogonal to the eigenvector\n",
    "    b_k = np.random.rand(A.shape[1])\n",
    "\n",
    "    for _ in range(num_simulations):\n",
    "        # calculate the matrix-by-vector product Ab\n",
    "        b_k1 = np.dot(A, b_k)\n",
    "\n",
    "        # calculate the norm\n",
    "        b_k1_norm = np.linalg.norm(b_k1)\n",
    "\n",
    "        # re normalize the vector\n",
    "        b_k = b_k1 / b_k1_norm\n",
    "\n",
    "    return b_k\n",
    "\n",
    "\n",
    "pi = power_iteration(M)\n",
    "pi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Algoritmo QR:\n",
    "<br>\n",
    "Em álgebra linear, o algoritmo QR é um algoritmo de autovalor: isso é, um procedimento para calcular os autovalores e autovetores de uma matriz. O algoritmo QR foi desenvolvido no fim dos anos 50 por John G. F. Francis e Vera N Kublanovskaya, trabalhando independentemente. A ideia básica é realizar a decomposição QR, escrevendo a matriz como um produto entre uma matriz ortogonal e uma matriz triangular superior, multiplicar os fatores em ordem reversa e iterar.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5         0.35355339 -0.57735027 -0.20412415]\n",
      " [-0.35355339 -0.25        0.          0.14433757]\n",
      " [-0.8660254   0.61237244  0.         -0.35355339]\n",
      " [ 0.20412415 -0.14433757  0.         -0.25      ]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 1.00000000e+00, -3.60726469e-01, -3.67680373e-01,\n",
       "         -2.07759703e-02],\n",
       "        [ 7.98610383e-27, -7.15621497e-02, -5.47157253e-01,\n",
       "         -4.59464994e-02],\n",
       "        [ 8.60048289e-27,  4.61398008e-01, -6.49684517e-01,\n",
       "         -2.19759990e-01],\n",
       "        [ 3.61351696e-56,  1.93857665e-30, -2.72966769e-30,\n",
       "         -2.78753333e-01]]),\n",
       " array([[ 0.72101012,  0.34317926,  0.58027437,  0.16016884],\n",
       "        [ 0.24033671, -0.87192667,  0.3005907 , -0.30270014],\n",
       "        [ 0.54075759,  0.14251356, -0.59759318, -0.57458986],\n",
       "        [ 0.36050506, -0.31884441, -0.46455277,  0.7433472 ]]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pure_qr(A, max_iter=50000):\n",
    "    Ak = np.copy(A)\n",
    "    n = A.shape[0]\n",
    "    QQ = np.eye(n)\n",
    "    for k in range(max_iter):\n",
    "        Q, R = np.linalg.qr(Ak)\n",
    "        Ak = R @ Q\n",
    "        QQ = QQ @ Q\n",
    "        if k % 100 == 0:\n",
    "            print(Ak)\n",
    "            print(\"\\n\")\n",
    "    return Ak, QQ\n",
    "\n",
    "\n",
    "pqr = pure_qr(M, 100)\n",
    "pqr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Abordagem em duas etapas\n",
    "Na abordagem em duas etapas para encontrar autovalores, seguimos as etapas:<br><br>\n",
    "1 - Reduzir a matriz para a forma de *Hessenberg* (zeros abaixo da primeira subdiagonal).<br>\n",
    "2 - Processo iterativo que faz Hessenberg convergir para uma matriz triangular. Os autovalores de uma matriz triangular são os valores da diagonal.\n",
    "<br><br>\n",
    "Na etapa 1, alcançamos a solução exata em um número finito de etapas, enquanto a fase 2 teoricamente nos dá a solução exata.\n",
    "<br><br>\n",
    "A etapa 2 é o próprio algoritmo QR. Lembrando que seria possível usar apenas o algoritmo QR, porém ele é extermamente lento.\n",
    "<br><br>\n",
    "Para a etapa 1, podemos usar o algoritmo de iteração de Arnoldi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.40969909,  0.89153222, -0.10656141, -0.16112609],\n",
       "        [ 0.01860023,  0.15820122,  0.94029881,  0.30077324],\n",
       "        [ 0.75805456, -0.26273255, -0.15516762,  0.57640949],\n",
       "        [ 0.50710351, -0.33333808,  0.28355919, -0.74251326]]),\n",
       " array([[ 0.79374457,  0.17930907,  0.65120523, -0.07606798],\n",
       "        [ 0.76992157, -0.51631042, -0.3138749 ,  0.21431793],\n",
       "        [ 0.        ,  0.39936577,  0.00182988, -0.00234396],\n",
       "        [ 0.        ,  0.        ,  0.00100315, -0.27926403]]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def arnoldi(A):\n",
    "    m, n = A.shape\n",
    "    assert (n <= m)\n",
    "\n",
    "    # Hessenberg matrix\n",
    "    H = np.zeros([n + 1, n])  # , dtype=np.float64)\n",
    "    # Orthonormal columns\n",
    "    Q = np.zeros([m, n + 1])  # , dtype=np.float64)\n",
    "    # 1st col of Q is a random column with unit norm\n",
    "    b = np.random.rand(m)\n",
    "    Q[:, 0] = b / np.linalg.norm(b)\n",
    "    for j in range(n):\n",
    "        v = A @ Q[:, j]\n",
    "        for i in range(j + 1):\n",
    "            # This comes from the formula for projection of v onto q.\n",
    "            # Since columns q are orthonormal, q dot q = 1\n",
    "            H[i, j] = np.dot(Q[:, i], v)\n",
    "            v = v - (H[i, j] * Q[:, i])\n",
    "        H[j + 1, j] = np.linalg.norm(v)\n",
    "        Q[:, j + 1] = v / H[j + 1, j]\n",
    "\n",
    "        # printing this to see convergence, would be slow to use in practice\n",
    "        # print(np.linalg.norm(A @ Q[:, :-1] - Q @ H))\n",
    "    return Q[:, :-1], H[:-1, :]\n",
    "\n",
    "\n",
    "Q0, H0 = arnoldi(M)\n",
    "Q0, H0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Aplicando ao PageRank\n",
    "<br>\n",
    "No PageRank aplicamos várias vezes a matriz de links ao vetor PageRank. Isso equivale à repetidamente multiplicar a matriz de links por si mesma e depois de suficientes auto-multiplicações, aplicar o resultado ao vetor PageRank.\n",
    "<br><br>Mas, multiplicação repetida de algo por si mesmo é a exponenciação. Então, nós podemos fazer isso exponenciando a matriz, o que pode ser feito através da exponenciação da diagonal da matriz centraldo resultado da decomposição, a qual contém os autovalores da matriz original.\n",
    "<br><br>\n",
    "Porém, o PageRank trabalha com uma matriz de milhões de valores, ou seja, até que o algoritmo se estabilize, existe um número imenso de iterações, ou seja de exponenciações da matriz de links. Assim, o maior autovalor passa a ser dominante na matriz, assim como a matriz de links final tem como vetor dominante um vetor associado com o maior autovalor, da mesma forma que o autovetor final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
